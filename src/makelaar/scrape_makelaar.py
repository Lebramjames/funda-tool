# %%
import os
import ast
import re
import pandas as pd
import numpy as np
import importlib.util
from typing import Optional
from datetime import date

from src.utils.get_url import get_html

def extract_scraper_metadata(directory: str, exclude: str = "scrape_makelaar_main_page.py") -> pd.DataFrame:
    """
    Extracts metadata (function name, URL, URL format) from all scraper scripts in a directory,
    excluding a specific file.

    Parameters:
        directory (str): Path to the directory containing Python scraper files.
        exclude (str): File to exclude from parsing.

    Returns:
        pd.DataFrame: Table with filename, function name, URL, and URL format.
    """
    results = []

    for filename in os.listdir(directory):
        if filename.endswith(".py") and filename != exclude:
            path = os.path.join(directory, filename)
            with open(path, "r", encoding="utf-8") as f:
                source = f.read()

            # Default values
            function_name = None
            url = None
            url_format = None

            # Extract function name
            try:
                tree = ast.parse(source)
                for node in tree.body:
                    if isinstance(node, ast.FunctionDef):
                        function_name = node.name
                        break
            except Exception:
                function_name = None

            # Extract __main__ block
            main_match = re.search(r'if\s+__name__\s*==\s*[\'"]__main__[\'"]\s*:(.*)', source, re.DOTALL)
            if main_match:
                main_block = main_match.group(1)

                url_match = re.search(r'url\s*=\s*[\'"]([^\'"]+)[\'"]', main_block)
                url_format_match = re.search(r'url_format\s*=\s*[\'"]([^\'"]+)[\'"]', main_block)

                if url_match:
                    url = url_match.group(1)
                if url_format_match:
                    url_format = url_format_match.group(1)

            results.append({
                "python_name": filename,
                "function_name": function_name,
                "url": url,
                "url_format": url_format
            })

    return pd.DataFrame(results)

def process_makelaars(
    df: pd.DataFrame,
    output_csv: str = None,
    num_pages: int = 5,
    row_limit: Optional[int] = 20
) -> pd.DataFrame:
    """
    Run makelaar scraper functions and store results in a single daily file.

    Parameters:
        df (pd.DataFrame): Must include 'python_name', 'function_name', 'url', 'url_format'
        output_csv (str): File path for the combined daily output CSV (default auto-generated by date)
        num_pages (int): Pagination depth
        row_limit (Optional[int]): Limit number of scrapers to run

    Returns:
        pd.DataFrame: Final combined results
    """
    from time import perf_counter
    time_start = perf_counter()

    today_str = date.today().strftime("%Y-%m-%d")
    if output_csv is None:
        output_csv = f"data/makelaars/makelaar_results_{today_str}.csv"

    os.makedirs(os.path.dirname(output_csv), exist_ok=True)

    # Step 1: Prepare df and generate URL lists
    df = df[df["url"].notna()].copy()

    def generate_url_list(row):
        urls = []
        if pd.notna(row['url']):
            urls.append(row['url'])
        if pd.notna(row['url_format']):
            if 'skip' in row['url_format']:
                for offset in np.arange(0, num_pages * 12, 12):
                    urls.append(row['url_format'].replace('{page}', str(offset)))
            else:
                for page in np.arange(2, 2 + num_pages):
                    urls.append(row['url_format'].replace('{page}', str(page)))
        return urls

    df['url_list'] = df.apply(generate_url_list, axis=1)

    # Step 2: Check already scraped python_files in today's file
    already_scraped = set()
    if os.path.isfile(output_csv):
        try:
            existing_df = pd.read_csv(output_csv)
            already_scraped = set(existing_df['python_file'].unique())
        except Exception as e:
            print(f"‚ö†Ô∏è Error reading {output_csv}: {e}")

    if row_limit:
        df = df.head(row_limit)

    combined_results = []

    for _, row in df.iterrows():
        if row['python_name'] in already_scraped:
            print(f"‚è≠ Skipping {row['python_name']} (already scraped today)")
            continue

        print(f"üîç Scraping {row['python_name']}...")

        module_path = os.path.join("src", "makelaar", row["python_name"])
        func_name = row["function_name"]
        url_list = row["url_list"]

        # Dynamic import
        spec = importlib.util.spec_from_file_location(func_name, module_path)
        module = importlib.util.module_from_spec(spec)
        try:
            spec.loader.exec_module(module)
        except Exception as e:
            print(f"‚ùå Failed to import {row['python_name']}: {e}")
            continue

        scraper_func = getattr(module, func_name, None)
        if not callable(scraper_func):
            print(f"‚ö†Ô∏è Function {func_name} not found or not callable")
            continue

        makelaar_results = []

        for url in url_list:
            try:
                html = get_html(url)
                data_list = scraper_func(html)

                if not isinstance(data_list, list):
                    print(f"‚ö†Ô∏è Function did not return list for {url}")
                    continue

                for data in data_list:
                    result_row = {
                        "python_file": row["python_name"],
                        "url": url
                    }
                    result_row.update(data)
                    makelaar_results.append(result_row)

            except Exception as e:
                print(f"‚ö†Ô∏è Error scraping {url}: {e}")

        if makelaar_results:
            print(f"‚úÖ Collected {len(makelaar_results)} rows from {row['python_name']}")
            combined_results.extend(makelaar_results)
        else:
            print(f"‚ö†Ô∏è No results from {row['python_name']}")

    # Step 3: Save combined data to single CSV
    if combined_results:
        new_df = pd.DataFrame(combined_results)

        if os.path.isfile(output_csv):
            try:
                existing_df = pd.read_csv(output_csv)
                final_df = pd.concat([existing_df, new_df], ignore_index=True).drop_duplicates()
            except Exception as e:
                print(f"‚ö†Ô∏è Failed reading {output_csv}: {e}")
                final_df = new_df
        else:
            final_df = new_df

        final_df.to_csv(output_csv, index=False)
        print(f"‚úÖ Saved total of {len(final_df)} rows to {output_csv}")
    else:
        print("‚ö†Ô∏è No new data scraped.")
        # return then the full DataFrame to avoid errors later
        final_df = pd.DataFrame()
        if os.path.isfile(output_csv):
            try:
                final_df = pd.read_csv(output_csv)
            except Exception as e:
                print(f"‚ö†Ô∏è Failed reading {output_csv}: {e}")
        return final_df

    print(f"‚è± Total time: {perf_counter() - time_start:.2f} seconds")
    return final_df

def run_makelaar_scraper():
    df = extract_scraper_metadata("src/makelaar/breakdown")
    today_str = date.today().strftime("%Y-%m-%d")
    output_csv = f"data/makelaars/makelaar_results_{today_str}.csv"
    results_df = process_makelaars(df, output_csv=output_csv, num_pages=5, row_limit=None)
    return results_df

if __name__ == "__main__":
    results_df = run_makelaar_scraper()
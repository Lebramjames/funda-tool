# %%
import os
import ast
import re
import pandas as pd

def extract_scraper_metadata(directory: str, exclude: str = "scrape_makelaar_main_page.py") -> pd.DataFrame:
    """
    Extracts metadata (function name, URL, URL format) from all scraper scripts in a directory,
    excluding a specific file.

    Parameters:
        directory (str): Path to the directory containing Python scraper files.
        exclude (str): File to exclude from parsing.

    Returns:
        pd.DataFrame: Table with filename, function name, URL, and URL format.
    """
    results = []

    for filename in os.listdir(directory):
        if filename.endswith(".py") and filename != exclude:
            path = os.path.join(directory, filename)
            with open(path, "r", encoding="utf-8") as f:
                source = f.read()

            # Default values
            function_name = None
            url = None
            url_format = None

            # Extract function name
            try:
                tree = ast.parse(source)
                for node in tree.body:
                    if isinstance(node, ast.FunctionDef):
                        function_name = node.name
                        break
            except Exception:
                function_name = None

            # Extract __main__ block
            main_match = re.search(r'if\s+__name__\s*==\s*[\'"]__main__[\'"]\s*:(.*)', source, re.DOTALL)
            if main_match:
                main_block = main_match.group(1)

                url_match = re.search(r'url\s*=\s*[\'"]([^\'"]+)[\'"]', main_block)
                url_format_match = re.search(r'url_format\s*=\s*[\'"]([^\'"]+)[\'"]', main_block)

                if url_match:
                    url = url_match.group(1)
                if url_format_match:
                    url_format = url_format_match.group(1)

            results.append({
                "python_name": filename,
                "function_name": function_name,
                "url": url,
                "url_format": url_format
            })

    return pd.DataFrame(results)

df = extract_scraper_metadata("src/makelaar")

import os
import re
import numpy as np
import pandas as pd
import importlib.util
from src.utils.get_url import get_html
from typing import Optional
from datetime import date

def run_makelaar_scrapers(
    df: pd.DataFrame,
    output_csv: str = None,
    num_pages: int = 5,
    row_limit: Optional[int] = 20
) -> pd.DataFrame:
    """
    Run makelaar scraper functions and store results in a single daily file.

    Parameters:
        df (pd.DataFrame): Must include 'python_name', 'function_name', 'url', 'url_format'
        output_csv (str): File path for the combined daily output CSV (default auto-generated by date)
        num_pages (int): Pagination depth
        row_limit (Optional[int]): Limit number of scrapers to run

    Returns:
        pd.DataFrame: Final combined results
    """
    from time import perf_counter
    time_start = perf_counter()

    today_str = date.today().strftime("%Y-%m-%d")
    if output_csv is None:
        output_csv = f"data/makelaars/makelaar_results_{today_str}.csv"

    os.makedirs(os.path.dirname(output_csv), exist_ok=True)

    # Step 1: Prepare df and generate URL lists
    df = df[df["url"].notna()].copy()

    def generate_url_list(row):
        urls = []
        if pd.notna(row['url']):
            urls.append(row['url'])
        if pd.notna(row['url_format']):
            if 'skip' in row['url_format']:
                for offset in np.arange(0, num_pages * 12, 12):
                    urls.append(row['url_format'].replace('{page}', str(offset)))
            else:
                for page in np.arange(2, 2 + num_pages):
                    urls.append(row['url_format'].replace('{page}', str(page)))
        return urls

    df['url_list'] = df.apply(generate_url_list, axis=1)

    # Step 2: Check already scraped python_files in today's file
    already_scraped = set()
    if os.path.isfile(output_csv):
        try:
            existing_df = pd.read_csv(output_csv)
            already_scraped = set(existing_df['python_file'].unique())
        except Exception as e:
            print(f"⚠️ Error reading {output_csv}: {e}")

    if row_limit:
        df = df.head(row_limit)

    combined_results = []

    for _, row in df.iterrows():
        if row['python_name'] in already_scraped:
            print(f"⏭ Skipping {row['python_name']} (already scraped today)")
            continue

        print(f"🔍 Scraping {row['python_name']}...")

        module_path = os.path.join("src", "makelaar", row["python_name"])
        func_name = row["function_name"]
        url_list = row["url_list"]

        # Dynamic import
        spec = importlib.util.spec_from_file_location(func_name, module_path)
        module = importlib.util.module_from_spec(spec)
        try:
            spec.loader.exec_module(module)
        except Exception as e:
            print(f"❌ Failed to import {row['python_name']}: {e}")
            continue

        scraper_func = getattr(module, func_name, None)
        if not callable(scraper_func):
            print(f"⚠️ Function {func_name} not found or not callable")
            continue

        makelaar_results = []

        for url in url_list:
            try:
                html = get_html(url)
                data_list = scraper_func(html)

                if not isinstance(data_list, list):
                    print(f"⚠️ Function did not return list for {url}")
                    continue

                for data in data_list:
                    result_row = {
                        "python_file": row["python_name"],
                        "url": url
                    }
                    result_row.update(data)
                    makelaar_results.append(result_row)

            except Exception as e:
                print(f"⚠️ Error scraping {url}: {e}")

        if makelaar_results:
            print(f"✅ Collected {len(makelaar_results)} rows from {row['python_name']}")
            combined_results.extend(makelaar_results)
        else:
            print(f"⚠️ No results from {row['python_name']}")

    # Step 3: Save combined data to single CSV
    if combined_results:
        new_df = pd.DataFrame(combined_results)

        if os.path.isfile(output_csv):
            try:
                existing_df = pd.read_csv(output_csv)
                final_df = pd.concat([existing_df, new_df], ignore_index=True).drop_duplicates()
            except Exception as e:
                print(f"⚠️ Failed reading {output_csv}: {e}")
                final_df = new_df
        else:
            final_df = new_df

        final_df.to_csv(output_csv, index=False)
        print(f"✅ Saved total of {len(final_df)} rows to {output_csv}")
    else:
        print("⚠️ No new data scraped.")
        return pd.DataFrame()

    print(f"⏱ Total time: {perf_counter() - time_start:.2f} seconds")
    return final_df

today_str = date.today().strftime("%Y-%m-%d")
output_csv = f"data/makelaars/makelaar_results_{today_str}.csv"
results_df = run_makelaar_scrapers(df, output_csv=output_csv, num_pages=5, row_limit=None)

# %%
import pandas as pd
import re

def prepare_address_fields(df: pd.DataFrame, full_address_col: str = "full_adres") -> pd.DataFrame:
    """
    Extracts street, number_extension, and city from a 'full_adres' column.
    Constructs a cleaned 'full_address_processed' column for geocoding.
    
    Parameters:
        df (pd.DataFrame): Input DataFrame containing a column with full addresses.
        full_address_col (str): Name of the column that contains the full address string.
        
    Returns:
        pd.DataFrame: The original DataFrame with added columns:
                      'street', 'number_extension', 'city', 'full_address_processed'
    """

    def split_address(addr):
        if not isinstance(addr, str):
            return pd.Series([None, None, None])
        
        city = None
        addr_wo_postal = addr
        if ' in ' in addr_wo_postal:
            city = addr_wo_postal.split(' in ')[-1].strip()
            addr_wo_postal = ' in '.join(addr_wo_postal.split(' in ')[:-1]).strip()
        elif ',' in addr_wo_postal:
            city = addr_wo_postal.split(',')[-1].strip()
            addr_wo_postal = ','.join(addr_wo_postal.split(',')[:-1]).strip()

        # Extract street and number
        match = re.search(r'(\D*?)(\d.*)', addr_wo_postal)
        if match:
            street = match.group(1).strip()
            number_extension = match.group(2).strip()
        else:
            street = addr_wo_postal.strip()
            number_extension = None

        return pd.Series([street, number_extension, city])

    # Apply address splitting
    df[['street', 'number_extension', 'city']] = df[full_address_col].apply(split_address)

    # Clean up number_extension and fill missing city
    df['number_extension'] = (
        df['number_extension']
        .replace('Amsterdam', '')
        .str.replace(r'\bAmsterdam\b', '', regex=True)
        .str.strip()
    )

    df['city'] = df['city'].fillna('Amsterdam')

    # Compose full address for geocoding
    df['full_address_processed'] = df.apply(
        lambda row: ' '.join(filter(None, [row['street'], row['number_extension'], row['city']])),
        axis=1
    )
    # df['full_address_streets'] here we add the street and city only
    df['full_address_streets'] = df.apply(
        lambda row: ' '.join(filter(None, [row['street'], row['city']])),
        axis=1
    )


    return df

results_df = prepare_address_fields(results_df, full_address_col="full_adres")
# find 

# %%
import os
import re
import pandas as pd
import pickle
from geopy.geocoders import Nominatim
from geopy.extra.rate_limiter import RateLimiter

def geocode_addresses_with_history(results_df: pd.DataFrame, cache_path="geo_cache.pkl", history_dir="data/makelaars") -> pd.DataFrame:
    # Initialize geocoder
    geolocator = Nominatim(user_agent="funda_scraper")
    geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)

    # Load geo_cache if exists
    if os.path.exists(cache_path):
        with open(cache_path, "rb") as f:
            geo_cache = pickle.load(f)
    else:
        geo_cache = {}

    # Step 1: Load all historical makelaar_scrape_output_*.csv files
    historical_coords = pd.DataFrame()
    for fname in os.listdir(history_dir):
        print(f"Processing file: {fname}")
        if re.match(r"makelaar_scrape_output_\d{4}-\d{2}-\d{2}\.csv", fname):
            try:
                path = os.path.join(history_dir, fname)
                df_hist = pd.read_csv(path)
                required_cols = {'full_address_processed', 'latitude', 'longitude'}
                if required_cols.issubset(df_hist.columns):
                    historical_coords = pd.concat([historical_coords, df_hist[list(required_cols)]], ignore_index=True)
                else:
                    print(f"Skipping {fname}: columns do not match.")
            except Exception as e:
                print(f"Could not read {fname}: {e}")

    # Step 2: Build from history into geo_cache
    required_cols = ['full_address_processed', 'latitude', 'longitude']
    if all(col in historical_coords.columns for col in required_cols):
        for _, row in historical_coords.dropna(subset=required_cols).iterrows():
            addr = row['full_address_processed']
            if addr not in geo_cache:
                geo_cache[addr] = (row['latitude'], row['longitude'])
    else:
        print("Warning: Not all required columns found in historical_coords, skipping cache build from history.")

    # Step 3: Define geocode function using cache
    def get_lat_lon(address):
        print(f"Geocoding address: {address}")
        if not isinstance(address, str) or address.strip() == "":
            return pd.Series([None, None])
        if address in geo_cache:
            return pd.Series(geo_cache[address])
        try:
            location = geocode(address)
            if location:
                geo_cache[address] = (location.latitude, location.longitude)
                return pd.Series([location.latitude, location.longitude])
        except Exception as e:
            print(f"Geocoding error for '{address}': {e}")
        geo_cache[address] = (None, None)
        return pd.Series([None, None])

    # Step 4: Get missing addresses only
    results_df["full_address_processed"] = results_df["full_address_processed"].astype(str)
    missing_latlon = results_df[
        results_df["full_address_processed"].notna() &
        ~results_df["full_address_processed"].isin(geo_cache.keys())
    ]["full_address_processed"].drop_duplicates()

    print(f"Geocoding {len(missing_latlon)} new addresses...")

    latlon_df = pd.DataFrame(missing_latlon, columns=['full_address_processed'])
    latlon_df[['latitude', 'longitude']] = latlon_df['full_address_processed'].apply(get_lat_lon)

    # Step 5: Combine back
    results_df = results_df.merge(latlon_df, on="full_address_processed", how="left")

    # Fill any missing from the cache (for addresses that existed already)
    results_df[['latitude', 'longitude']] = results_df.apply(
        lambda row: pd.Series(geo_cache.get(row['full_address_processed'], (row.get('latitude'), row.get('longitude')))),
        axis=1
    )
    return results_df

results_df = geocode_addresses_with_history(results_df)

# %% Final prints: 
